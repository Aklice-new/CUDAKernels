# CUDAKernels

Implement efficient operators in deep learning network with cuda.

## Element kernels

### unary ops
RELU,GELU,Sqrt,Sigmoid...

### binary ops
Add,Sub,Mul,Div...

1. å¯¹äºè¿™ç§elementwiseçš„ç®—å­ï¼Œæœ€æœ´ç´ çš„å®ç°æ–¹å¼æ˜¯å¯¹æ¯ä¸ªå…ƒç´ å¼€ä¸€ä¸ªçº¿ç¨‹å»è¿›è¡Œè®¡ç®—ã€‚
2. å¦ä¸€ä¸­é«˜æ•ˆçš„å®ç°å°±æ˜¯æœ‰æ•ˆçš„åˆ©ç”¨cudaåœ¨è®¿å­˜è¿‡ç¨‹ä¸­çš„åˆå¹¶è®¿å­˜(coalesced memory access)è¿™ä¸ªç‰¹æ€§ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯åœ¨ä¸€ä¸ªwarpå†…çš„çº¿ç¨‹è®¿é—®å…¨å±€å†…å­˜æ—¶ï¼Œä¼šè¿›è¡Œ(32-bit)çš„åˆå¹¶è®¿å­˜ï¼Œå¯¹äºè¿ç»­çš„32-bitçš„è®¿å­˜ï¼Œåªéœ€è¦ä¸€æ¬¡è®¿å­˜æ“ä½œï¼Œå¯¹warpå†…çš„çº¿ç¨‹éƒ½æ˜¯å¯è§çš„ã€‚è¿™æ ·å°±å¯ä»¥å‡å°‘è®¿å­˜çš„æ¬¡æ•°ï¼Œæé«˜è®¿å­˜çš„æ•ˆç‡ã€‚
3. è¿˜æœ‰å¯ä»¥é€šè¿‡LDG.128ï¼ŒSTS.128æ¥æé«˜è®¿å­˜çš„æ•ˆç‡ã€‚è¿™é‡Œåœ¨llm.cä¸­çš„å®ç°è¿‡ç¨‹ä¸­æ„é€ äº†ä¸€ä¸ªPacked128çš„æ•°æ®ç±»å‹ï¼Œç”¨äºæ‰“åŒ…128-bitçš„æ•°æ®æ¥æ–¹ä¾¿è¿›è¡Œæ“ä½œã€‚

### å®éªŒç»“æœ
é€šè¿‡LDG.128å¹¶æ²¡æœ‰å¯¹å¸¦å®½æœ‰å¾ˆå¤§æå‡ï¼Œåè€Œä¼šé™ä½æœ‰æ•ˆå¸¦å®½

## Softmax kernel

[How to write a fast softmax cuda kernel -AITemplate(facebook) ](https://github.com/facebookincubator/AITemplate/wiki/How-to-write-a-fast-Softmax-CUDA-kernel%3F)

[register cache warp cuda](https://developer.nvidia.com/blog/register-cache-warp-cuda/)

Softmaxçš„å…¬å¼å¦‚ä¸‹ï¼š 
$$
Softmax(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^{n}\exp{x_j}}
$$
ä¸ºäº†ä¿è¯æ•°å€¼çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¬å¼è¿›è¡Œä¸€äº›å˜æ¢ï¼š
$$
Softmax(x_i) = \frac{\exp(x_i - \max(x_m))}{\sum_{j=1}^{n}\exp(x_j - \max(x_m))}
$$

åœ¨å®é™…çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œéœ€è¦é¦–å…ˆè®¡ç®—maxï¼Œç„¶åå‡å»maxï¼Œç„¶åè®¡ç®—expï¼Œå¹¶æ±‚å’Œsumï¼Œéœ€è¦ä¸‰æ¬¡forå¾ªç¯ï¼Œå¯¹äºä¸€ä¸ªå…ƒç´ æ¥è¯´ï¼Œéœ€è¦3æ¬¡loadå’Œ
ä¸€æ¬¡storeï¼Œè¿™æ ·å¯¹è®¿å­˜çš„å‹åŠ›å¾ˆå¤§ï¼Œæ‰€ä»¥æœ‰åé¢çš„online softmaxçš„ä¼˜åŒ–æ–¹æ³•ã€‚

### online softmax
[online softmax paper](https://arxiv.org/pdf/1805.02867)
![Alt text](assets/softmax.png)

![Alt text](assets/online_softmax.png)

ä¸¤è€…çš„åŒºåˆ«åœ¨äºç¬¬äº”æ­¥ï¼Œé€šè¿‡æ•°å­¦çš„æ¨å¯¼ï¼Œå¯ä»¥åœ¨æ›´æ–°maxçš„è¿‡ç¨‹ä¸­è®¡ç®—expã€‚
![Alt text](assets/online_softmax_details.png)

å‡å°‘ä¸€æ¬¡loadçš„æ“ä½œï¼Œå‡å°‘äº†è®¿å­˜çš„å‹åŠ›ï¼Œæé«˜äº†è®¡ç®—çš„æ•ˆç‡ã€‚


### cuda kernel å®ç°


## layer normalization kernel
[layernorm](https://arxiv.org/pdf/1607.06450.pdf)

layer norm å¯¹æ¯ä¸€æ¬¡çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œç„¶åè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
$$
y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
å…¶ä¸­$\mu$æ˜¯å‡å€¼ï¼Œ$\sigma$æ˜¯æ–¹å·®ï¼Œ$\gamma$å’Œ$\beta$æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œ$\epsilon$æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨äºé˜²æ­¢åˆ†æ¯ä¸º0ã€‚
è¾“å…¥æ˜¯B,T,Cçš„tensorï¼Œå¯¹äºæ¯ä¸€ä¸ªCçš„channelï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå¯¹æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œå½’ä¸€åŒ–ã€‚

### kernelå®ç°
å¯¹äºæ¯ä¸€ä¸ªchannelï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå¯¹æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œå½’ä¸€åŒ–ã€‚
éœ€è¦è®¡ç®—ä¸€æ¬¡å‡å€¼å’Œæ–¹å·®ï¼Œå…ˆè®¡ç®—å‡å€¼ï¼Œç„¶åè®¡ç®—æ–¹å·®ï¼Œç„¶åå¯¹æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œå½’ä¸€åŒ–ã€‚
æˆ–è€…é€šè¿‡æ–¹å·®çš„å…¬å¼ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—æ–¹å·®ã€‚
è®¡ç®—å‡å€¼å’Œæ–¹å·®å°±æ˜¯ä¼ ç»Ÿçš„reduceæ“ä½œï¼Œå¯ä»¥é€šè¿‡ä¸åŒlevelçš„Reduceæ¥å®ç°ã€‚

## self-attention kernel
[self-attention paper](https://arxiv.org/pdf/1706.03762.pdf)

self-attentionæ˜¯transformerçš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡è®¡ç®—query,key,valueçš„**å†…ç§¯**ï¼Œç„¶åé€šè¿‡softmaxå¾—åˆ°attentionçš„æƒé‡ï¼Œç„¶åå¯¹valueè¿›è¡Œ**å†…ç§¯**ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

$$
Attention = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

å…¶ä¸­
Q : [B, T, D_K]
K : [B, T, D_K]
V : [B, T, D_V]


```python
# Self Attention
import torch
import torch.nn
from math import sqrt

class SelfAttention(nn.Module):
    # input : B * T * D
    # Q     : B * D * D_K
    # K     : B * D * D_K
    # V     : B * D * D_V
    def __init(self, input_dim, dim_k, dim_v):
        super(SelfAttention, self).__init__()
        self.Q = nn.Linear(input_dim, dim_k)
        self.K = nn.Linear(input_dim, dim_k)
        self.V = nn.Linear(input_dim, dim_v)
        self._norm_factor = 1.0 / sqrt(dim_k)
    
    def forward(self, x):
        q = self.Q(x) # B * T * D_K
        k = self.K(x) # B * T * D_K
        v = self.V(x) # B * T * D_V
        
        attention = torch.bmm(q, k.permute(0, 2, 1)) # B * T * T
        attention = nn.Softmax(dim=-1)(attention) * self._norm_factor # B * T * T

        output = torch.bmm(attention, v) # B * T * D_V

        return output

# Multi-head Attention
class MultiHeadAttention(nn.Module):
    # input : B * T * D
    # Q     : B * D * D_K  // num_heads
    # K     : B * D * D_K // num_heads
    # V     : B * D * D_V // num_heads
    def __init__(self, input_dim, dim_k, dim_v, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.Q = nn.Linear(input_dim, dim_k)
        self.K = nn.Linear(input_dim, dim_k)
        self.V = nn.Linear(input_dim, dim_v)
        self.head_size = dim_k // num_heads
        self.num_heads = num_heads
        self.dim_k = dim_k
        self.dim_v = dim_v
        self._norm_factor = 1.0 / sqrt(head_size)

    def forward(self, x):
        q = self.Q(x).view(B, T, num_heads, dim_k // self.num_heads).permute(0, 2, 1, 3) #[B, num_heads, T, D_K]
        k = self.K(x).view(B, T, num_heads, dim_k // self.num_heads).permute(0, 2, 1, 3) #[B, num_heads, T, D_K]
        v = self.V(x).view(B, T, num_heads, dim_v // self.num_heads).permute(0, 2, 1, 3) #[B, num_heads, T, D_K]

        attention = torch.bmm(q, k.permute(-1, -2)) # [B, num_heads, T, T]
        attention = nn.Softmax(dim=-1)(attention) * self._norm_factor # B * num_heads * T * T

        output = torch.bmm(attention, v) # B * num_heads * T * head_size
        output = output.view(B, T, dim_v) # B * T * D_V

        return output


```

## Flash Attention


ä¸€äº›å‚è€ƒå­¦ä¹ çš„é“¾æ¥:
[[Attentionä¼˜åŒ–][2wå­—]ğŸ”¥åŸç†&å›¾è§£: ä»Online-Softmaxåˆ°FlashAttention V1/V2/V3](https://www.cvmart.net/community/detail/14806)
